{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter11_part01_introduction.ipynb","provenance":[],"collapsed_sections":["zapSmyeMkLUb","IYnVquAriBkr","rnM3GO5ziBks","VKoW4o99iBks"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zapSmyeMkLUb"},"source":["# Harper Adams Data Science \n","## NLP deeper dive\n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/img/HAP-E-logo.png?raw=true\" alt=\"HAP-E Group\" width=\"125\"/>\n","</center>\n","\n","Ed Harris </br>\n","HARUG! / HAP-E Group </br>\n","2021-12-01 </br></br>\n","\n","---\n","\n","This notebook is adapted from Ch 11 from:</br> \n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/Chollet.jpg?raw=true\" alt=\"HAP-E Group\" width=\"125\"/>\n","</center>\n","Chollet, F., 2021. Deep Learning with Python, 2nd ed. Manning Publications, Shelter Island, NY.\n","</br></br>\n","\n","Also see:\n","\n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/Liu.png?raw=true\" alt=\"HAP-E Group\" width=\"125\"/>\n","\n","\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/Rothman.png?raw=true\" alt=\"HAP-E Group\" width=\"125\"/>\n","</center>\n","\n","Liu, B., 2020. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions, 2nd ed. Cambridge University Press.\n","\n","Rothman, D., 2021. Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more, 1st ed. Packt Publishing.\n","\n","&nbsp;\n","---\n","\n",">Frederick Jelinek, an early speech recognition researcher,\n","joked in the 1990s: “Every time I fire a linguist, the performance of the speech recognizer\n","goes up.”\n"]},{"cell_type":"markdown","metadata":{"id":"cWOInHiliBkq"},"source":["# Deep learning for text"]},{"cell_type":"markdown","metadata":{"id":"sH4UkvZuiBkr"},"source":["## 11.1 Natural-language processing: The bird's eye view\n","\n","- “What’s the topic of this text?” (text classification)\n","- “Does this text contain abuse?” (content filtering)\n","- “Does this text sound positive or negative?” (sentiment analysis)\n","- “What should be the next word in this incomplete sentence?” (language modeling)\n","- “How would you say this in German?” (translation)\n","- “How would you summarize this article in one paragraph?” (summarization)"]},{"cell_type":"markdown","metadata":{"id":"uUMWUpbbiBkr"},"source":["## 11.2 Preparing text data\n","\n",">...keep in mind... the text-processing models you will train won’t possess a human-like understanding of language; rather, they simply look for statistical regularities in their input data, which turns out to be sufficient to perform well on many simple tasks.\n","\n","</br></br>\n","\n","- First, you standardize the text to make it easier to process, such as by converting\n","it to lowercase or removing punctuation.\n","- You split the text into units (called tokens), such as characters, words, or groups\n","of words. This is called tokenization.\n","- You convert each such token into a numerical vector. This will usually involve\n","first indexing all tokens present in the data.\n","\n","</br></br>\n","\n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/11-1.png?raw=true\" alt=\"Prepping text data steps\" width=\"600\"/>\n","</center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IYnVquAriBkr"},"source":["### Text standardization\n","\n","This involves removing punctualtion and formatting, and also subtle differences such as verb tense and similar language ambiguities.\n","\n","&nbsp;\n","\n","Consider:\n","\n","- “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”\n","- “Sunset came; I stared at the México sky. Isn’t nature splendid?”\n","\n","&nbsp;\n","\n","Remove punctuation and capitals:\n","\n","- “sunset came i was staring at the mexico sky isnt nature splendid”\n","- “sunset came i stared at the méxico sky isnt nature splendid”\n","\n","&nbsp;\n","\n","'Stemming' (standardize variation of terms)\n","\n","- “sunset came i [stare] at the mexico sky isnt nature splendid”\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rnM3GO5ziBks"},"source":["### Text splitting (tokenization)\n","\n","\n","&nbsp;\n","\n",">Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called tokenization. \n","\n","- Word-level tokenization: Where tokens are space-separated (or punctuationseparated) substrings. A variant of this is to further split words into subwords when applicable. For instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n","- N-gram tokenization: Where tokens are groups of N consecutive words. For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n","- Character-level tokenization: Where each character is its own token. In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition.\n","</br></br>\n","\n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/11-2.png?raw=true\" alt=\"Prepping text data steps\" width=\"600\"/>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"VKoW4o99iBks"},"source":["### Vocabulary indexing\n","\n",">Once your text is split into tokens, you need to encode each token into a numerical representation.\n","\n","Build an index of all terms found in the training data (the “vocabulary”), and assign a unique integer to each entry in the vocabulary."]},{"cell_type":"markdown","metadata":{"id":"q5w2NryGiBks"},"source":["### Using the TextVectorization layer\n"]},{"cell_type":"code","metadata":{"id":"AnO-7mE7iBkt"},"source":["# Make a set of functions for vocabulary indexing\n","# (the hard way just, to illustrate)\n","\n","import string\n","\n","class Vectorizer:\n","    def standardize(self, text):\n","        text = text.lower()\n","        return \"\".join(char for char in text if char not in string.punctuation)\n","\n","    def tokenize(self, text):\n","        text = self.standardize(text)\n","        return text.split()\n","\n","    def make_vocabulary(self, dataset):\n","        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n","        for text in dataset:\n","            text = self.standardize(text)\n","            tokens = self.tokenize(text)\n","            for token in tokens:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)\n","        self.inverse_vocabulary = dict(\n","            (v, k) for k, v in self.vocabulary.items())\n","\n","    def encode(self, text):\n","        text = self.standardize(text)\n","        tokens = self.tokenize(text)\n","        return [self.vocabulary.get(token, 1) for token in tokens]\n","\n","    def decode(self, int_sequence):\n","        return \" \".join(\n","            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n","\n","vectorizer = Vectorizer()\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","vectorizer.make_vocabulary(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2n-h5nxciBku"},"source":["# test your encoding function\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = vectorizer.encode(test_sentence)\n","print(encoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SWODxwkiBku"},"source":["# decoded, and unknown values\n","decoded_sentence = vectorizer.decode(encoded_sentence)\n","print(decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AORnoJ8I0N7D"},"source":["> In practice, most people would work with the Keras TextVectorization layer, which is fast and efficient and can be dropped directly into a tf.data pipeline or a Keras model.\n","\n","This is what the TextVectorization layer looks like:"]},{"cell_type":"code","metadata":{"id":"81HC0XjQiBkv"},"source":["from tensorflow.keras.layers import TextVectorization\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",        # argument output text to integer\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eENolAt8iBkv"},"source":["# some service libraries + tensorflow\n","\n","import re\n","import string\n","import tensorflow as tf\n","\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = tf.strings.lower(string_tensor) # convert strings to lowercase\n","    return tf.strings.regex_replace(                 # replace punctuation with empty string\n","        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") \n","\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor) # split string on whitespace\n","\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n","    standardize=custom_standardization_fn,\n","    split=custom_split_fn,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBtJfPN4iBkv"},"source":["# test it\n","# also you can play and experiment with this by editing the dictionary...\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","text_vectorization.adapt(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZpst1xziBkv"},"source":["**Displaying the vocabulary**"]},{"cell_type":"code","metadata":{"id":"0K-_eKBkiBkw"},"source":["# notice the symbolic [UNK]\n","text_vectorization.get_vocabulary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7hKWeASiBkw"},"source":["# test sentence\n","vocabulary = text_vectorization.get_vocabulary()\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = text_vectorization(test_sentence)\n","print(encoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBJJfnRriBkw"},"source":["# decode back from int\n","inverse_vocab = dict(enumerate(vocabulary))\n","decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n","print(decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBkA8FroiBkw"},"source":["## 11.3 Two approaches for representing groups of words: Sets and sequences\n","\n","The problem of word order is a principal challenge in Natrual Language Processing\n","\n","Consider:\n","\n","Regular: You will take out the trash\n","\n","Yoda:    The trash you will take out\n","\n","approach 1: ignore order (e.g. 'bag of words')\n","\n","approach 2: consider order (e.g. RNN, Transformers)\n","\n","Here we will look at sentiment useing each approach using the famous IMDB Movie Review Dataset."]},{"cell_type":"markdown","metadata":{"id":"VDuma97EiBkw"},"source":["### Preparing the IMDB movie reviews data"]},{"cell_type":"code","metadata":{"id":"fW2dXJ9MiBkx"},"source":["# Get the data - courtesy of Andrew Maas, Stanford U\n","!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"78CBJGwjiBkx"},"source":["# Delete and unneeded part of the data (optional... haha)\n","!rm -r aclImdb/train/unsup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aB-VvScY-szf"},"source":["# ls aclImdb/train/pos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZvuTHMaiBkx"},"source":["# !cat aclImdb/train/pos/4077_10.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYNOh4DU_C1H"},"source":["!cat aclImdb/train/pos/10000_8.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMCULFmCiBkx"},"source":["# Create train / test / validate partitions\n","# only run once, visually inspect dir structure if unsure\n","\n","import os, pathlib, shutil, random\n","\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxVopFl7iBkx"},"source":["# set location of train / test / validate files\n","# the 2 classes are negative versus positive\n","\n","from tensorflow import keras\n","batch_size = 32\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OFEslnwiBkx"},"source":["**Displaying the shapes and dtypes of the first batch**"]},{"cell_type":"code","metadata":{"id":"o9eHBH5oiBkx"},"source":["# These datasets yield inputs that are TensorFlow tf.string tensors and targets that are\n","# int32 tensors encoding the value “0” or “1.”\n","for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkfUEjTRiBky"},"source":["### Processing words as a set: The bag-of-words approach"]},{"cell_type":"markdown","metadata":{"id":"xERdgO1wiBky"},"source":["#### Single words (unigrams) with binary encoding\n","\n","If you use a bag of single words, the sentence “the cat sat on the mat” becomes\n","\n","{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n","\n","The main advantage of this encoding is that you can represent an entire text as a single vector (e.g. there is no order or context imformation)."]},{"cell_type":"markdown","metadata":{"id":"J5BRnY6uiBky"},"source":["**Preprocessing our datasets with a `TextVectorization` layer**"]},{"cell_type":"code","metadata":{"id":"f_XdqLxTiBky"},"source":["# Practicality: Limit the vocabulary to the 20,000 most frequent words.\n","text_vectorization = TextVectorization(\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")\n","\n","# Prepare a dataset that only yields raw text inputs (no labels)\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","\n","# Use that dataset to index the dataset vocabulary via the adapt() method\n","text_vectorization.adapt(text_only_train_ds)\n","\n","# Prepare processed versions of our training, validation, and test dataset.\n","# NB you can specify num_parallel_calls to leverage multiple CPUs!\n","binary_1gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17wvBnChiBky"},"source":["**Inspecting the output of our binary unigram dataset**"]},{"cell_type":"code","metadata":{"id":"NOqsHGtUiBky"},"source":["for inputs, targets in binary_1gram_train_ds:\n","    print(\"inputs.shape:\", inputs.shape) # 32bit integer, 20000 possible words\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape) \n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0]) # just 0 or 1 for presence against the 20K\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2F1Z6ULiBky"},"source":["**Our model-building utility**"]},{"cell_type":"code","metadata":{"id":"tgiYFu4oiBky"},"source":["# Uses popular and mainstream Tensorflow and Keras\n","# to set up our language 'model'\n","# need some neural network background here\n","# but we will ignore this...\n","\n","# This will results in a 'model' definition\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def get_model(max_tokens=20000, hidden_dim=16):\n","    inputs = keras.Input(shape=(max_tokens,))\n","    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n","    x = layers.Dropout(0.5)(x)\n","    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer=\"rmsprop\",\n","                  loss=\"binary_crossentropy\",\n","                  metrics=[\"accuracy\"])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XzI2pYvFiBkz"},"source":["**Training and testing the binary unigram model**"]},{"cell_type":"code","metadata":{"id":"4OL9wVWjiBkz"},"source":["# this will take a few minutes to run (progress bar)\n","\n","# 'Calls' and preps the model we just defined\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n","                                    save_best_only=True)\n","]\n","\n","# this runs the model against our data\n","model.fit(binary_1gram_train_ds.cache(), #NB this caches the data in memory: called once\n","          validation_data=binary_1gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","\n","# stores the model\n","model = keras.models.load_model(\"binary_1gram.keras\")\n","\n","# print the results - should be pretty good... like > 80% accuracy in testing\n","print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUA2BG53iBkz"},"source":["#### Bigrams with binary encoding\n","\n","Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately.\n","\n","For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams).\n","\n","&nbsp;\n","\n","With bigrams, our sentence becomes\n","\n","{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n","\n","\"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n","\n","&nbsp;\n","\n","The TextVectorization layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an ngrams=N argument as in the following listing."]},{"cell_type":"markdown","metadata":{"id":"942Q_pC5iBkz"},"source":["**Configuring the `TextVectorization` layer to return bigrams**"]},{"cell_type":"code","metadata":{"id":"VQ9XM6iTiBkz"},"source":["text_vectorization = TextVectorization(\n","    ngrams=2,     # specifies 2-grams\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMYcY2SliBkz"},"source":["**Training and testing the binary bigram model**"]},{"cell_type":"code","metadata":{"id":"BKVqudSAiBkz"},"source":["# rerun 2-gram model and print results\n","# should take a few mins\n","# DO YOU THINK this will do better, poorer, or about the same?\n","\n","text_vectorization.adapt(text_only_train_ds)\n","binary_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(binary_2gram_train_ds.cache(),\n","          validation_data=binary_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"binary_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5GXJbVyiBkz"},"source":["#### Bigrams with TF-IDF encoding\n","\n","You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:\n","\n","{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n","\n","\"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n","\n","\n","&nbsp;\n","\n","If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is likely a negative one."]},{"cell_type":"markdown","metadata":{"id":"EFa12S-7iBkz"},"source":["**Configuring the `TextVectorization` layer to return token counts**"]},{"cell_type":"code","metadata":{"id":"jiB9dW2giBkz"},"source":["# count occurrence\n","text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"count\"  # count occurrence\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YPB60_liBk0"},"source":["**Configuring `TextVectorization` to return TF-IDF-weighted outputs**\n","\n","Some important technical detail to UNDERSTAND, but it will work even if IGNORED (ha!).\n","\n","Now, of course, some words are bound to occur more often than others no matter what the text is about. The words “the,” “a,” “is,” and “are” will always dominate your word count histograms, drowning out other words—despite being pretty much useless features in a classification context. \n","\n","&nbsp;\n","\n","How could we address this?: via 'normalization'. \n","\n","&nbsp;\n","\n","We could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19,988 zero entries), a property called “sparsity.” That’s a great property to have, as it dramatically reduces compute load and reduces the risk of overfitting. If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use\n","should be divide-only. \n","\n","What, then, should we use as the denominator? The best practice\n","is to go with something called TF-IDF normalization—TF-IDF stands for “term frequency, inverse document frequency.”"]},{"cell_type":"code","metadata":{"id":"xNYopmx7iBk0"},"source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"tf_idf\", #set TF-IDF mode\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15n58qMKiBk0"},"source":["**Training and testing the TF-IDF bigram model**"]},{"cell_type":"code","metadata":{"id":"bx1IXiGWiBk0"},"source":["# setup model and run like before\n","# takes a few mins\n","text_vectorization.adapt(text_only_train_ds)\n","\n","tfidf_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","\n","# NB adding model fit to object 'history'\n","history = model.fit(tfidf_2gram_train_ds.cache(),\n","          validation_data=tfidf_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"tfidf_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrUIRl9cNHm9"},"source":["# for plotting\n","import matplotlib.pyplot as plt\n","import numpy\n","\n","# list all data in history\n","print(history.history.keys())\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Auf3xFIkd7Br"},"source":["#### Testing our model on novel data\n","\n","<center>\n","   <img src=\"https://github.com/ha-data-science/ha-data-science.github.io/blob/main/pages/harug-files/2021-12-01-NLP-sentiment-analysis/img/WoT.jpg?raw=true\" alt=\"Wheel of Time starring Rosamund Pike\" width=\"500\"/>\n","</center>"]},{"cell_type":"code","metadata":{"id":"gvObVM8ziBk0"},"source":["# set up inputs to actually test our model on novel data\n","inputs = keras.Input(shape=(1,), dtype=\"string\")\n","processed_inputs = text_vectorization(inputs)\n","outputs = model(processed_inputs)\n","inference_model = keras.Model(inputs, outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQTeI22iiBk0"},"source":["# test\n","\n","my_Fake_Review0 = \"The actress was terrible and the writing was bad\"\n","\n","my_Fake_Review1 = \"The actress was relatable and the writing was fabulous\"\n","\n","# Amazon wheel of time from today, literally\n","# 2 stars\n","real_Review0 = \"I have read all of the books cover to cover at least 4 times, but I am judging this as a standalone effort. To be Blunt, it is bad. We start with a big bit of drama in episode 1 and very little reason to care. As it continues into the 3rd episode the characters are so utterly devoid of any personality that there's still little reason to care. Pike chews the set with some of the worst acting I have ever seen. The rest of the cast is not much better. The script is as subtle as a hammer blow. It is childish, unnatural and lacking any art. The CGI is pretty and the monsters are genuinely scary to look at, but then it goes too far with Styrofoam bricks flying around. Touching a little upon the books you have a world where all races unite for a final battle and a world mostly run by women. Despite this, the writers have chosen to go OTT with some aspects of equality which completely devalues the equality the books naturally crafted. There is no art here (beyond set design, makeup, costume and some CGI). It is a bloated mess filled with bland characters in a constant state of reaction. There is no flow, no rhythm and no heart. It is a hollow big budget waste, sadly. Looking squarely at the books they have changed a lot of things. The journey is much the same, but Mat is not Mat, the Aes Sedai are just weird, Lan is a skinny dude, and there are small changes here and there that just weren't needed. If it was like this and good, I'd have loved it. I wouldn't mind the series and the books having their own identity. Sadly, this is a poor childish hackjob of a creation that failed to hold my interest in any meaningful way. New Spring and. The Eye of the World captivated me, made me care instantly for the people and held me for the whole series. This. This did not.\"\n","\n","# 5 stars\n","real_Review1 = \"Seems their are two parties who have watched this, the ones that are new, book readers who went in with an open mind and expected changes, and then the ones who went in with a closed mind giving 1 or 2 star reviews. The acting the scenery even seeing the one power wielderld (which I was a bit skeptical about in the trailers) was fantastic, I'm a massive fan of the books and I expected changed but nothing so far I feel has taken away from the overarching story. For open minded fans and people new to the wheel of time story this is a must watch!!!\"\n","\n","import tensorflow as tf\n","raw_text_data = tf.convert_to_tensor([\n","    [my_Fake_Review0],\n","])\n","\n","predictions = inference_model(raw_text_data)\n","print(f\"{float(predictions[0] * 100):.2f} percent positive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gF9R0_HvbDQo"},"source":[""],"execution_count":null,"outputs":[]}]}